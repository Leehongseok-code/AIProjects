import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime
import torch.optim as optim

torch.manual_seed(1)

#data loading
data=pd.read_csv('Data/csvdata/005930.Ks.csv')

#mid price computing
high=data['High'].values
low=data['Low'].values
mid=(high+low)/2

def minmax(data):
    numerator=data-min(data)
    denominator=max(data)-min(data)
    return numerator/(denominator+1e-7)

def build_dataset(time_series,seq_length):#(train_data,7)
    datax=[]
    datay=[]
    for i in range(0,len(time_series)-seq_length):
        #print(time_series.shape)
        tempx=[]
        for j in range(i,i+seq_length):
            tempx.append([time_series[j]])
        _x=tempx
        #print(_x)
        #_x=time_series[i:i+seq_length,:]
        _y=time_series[[i+seq_length]]#다음 날 주가
        print(_x,"->",_y)
        datax.append(_x)
        datay.append(_y)
    return np.array(datax),np.array(datay)

seq_length=7#한 번에 일주일치 데이터를 기반으로 다음 날 데이터 예측
input_dim=1#주는 데이터는 가격 한 가지이므로 1차원 벡터,이후에 더 정보를 준다면 n차원 벡터일 것
#output은 다음 날 주가 정보 하나이지만, hidden_dim=1이면 예측에 도움을 줄 수 있는 정보 사이즈 부족
hidden_dim=10
output_dim=1#fc를 연결해서 10차원을 1차원으로 압축할 것임
learning_rate=0.01
iterations=500
xy=mid
mid=mid[::-1]#reverse

train_size=int(len(xy)*0.8)
train_data=xy[0:train_size]
test_data=xy[train_size-seq_length:]
train_data=minmax(train_data)
test_data=minmax(test_data)
#print(train_data[500])

trainx,trainy=build_dataset(train_data,seq_length)
testx,testy=build_dataset(test_data,seq_length)

trainx_tensor=torch.FloatTensor(trainx)
trainy_tensor=torch.FloatTensor(trainy)

testx_tensor=torch.FloatTensor(testx)
testy_tensor=torch.FloatTensor(testy)

class Net(torch.nn.Module):
    def __init__(self,input_dim,hidden_dim,output_dim,layers):
        super(Net,self).__init__()
        self.rnn=torch.nn.LSTM(input_dim,hidden_dim,num_layers=layers,batch_first=True)
        # 10차원의 hidden layer를 단일 차원의 outpput layer로 변환
        self.fc=torch.nn.Linear(hidden_dim,output_dim,bias=True)

    def forward(self,x):
        x,_status=self.rnn(x)
        x=self.fc(x[:,-1])
        return x

model=Net(input_dim,hidden_dim,output_dim,1)

#loss&optimizer setting
criterion=torch.nn.MSELoss()
optimizer=optim.Adam(model.parameters(),lr=learning_rate)

#training
for epoch in range(iterations):
    optimizer.zero_grad()
    #print(trainx_tensor.shape)
    outputs=model(trainx_tensor)
    cost=criterion(outputs,trainy_tensor)
    cost.backward()
    optimizer.step()
    print(epoch,cost.item())
    #print(trainx_tensor.shape,',',trainy_tensor.shape)

print(model(testx_tensor)[0][0])
#print("prediction:",model(testx_tensor))
#print("testy:",testy_tensor)

for i in range(500):#주식자료를 이어붙여서 예측하게 할거야
    #마지막으로 예측한 값을 뒤로 이어붙여서 새로운 자료를 만듦(미지의 자료를 예측하게함)
    appender_float=float(model(testx_tensor)[len(model(testx_tensor))-1][0])
    #print(float(appender_float))
    # testy_data-original의 길이는 더 늘리지 않고, prediction만 늘릴 것임
    testx,tempdata=build_dataset(np.append(test_data,appender_float),seq_length)
    testx_tensor = torch.FloatTensor(testx)

plt.plot(testy)
plt.plot(model(testx_tensor).data.numpy())
plt.legend(['original','prediction'])
plt.show()
